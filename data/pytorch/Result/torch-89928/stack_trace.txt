Traceback (most recent call last):
  File "torch-89928-original.py", line 26, in <module>
    fx.test_full_backward_hook_double_backward()
  File "torch-89928-original.py", line 22, in test_full_backward_hook_double_backward
    torch.autograd.grad(gradx_f, x)
  File "/root/anaconda3/envs/torch-1.13.0/lib/python3.8/site-packages/torch/autograd/__init__.py", line 300, in grad
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/root/anaconda3/envs/torch-1.13.0/lib/python3.8/site-packages/torch/utils/hooks.py", line 101, in hook
    raise RuntimeError("Module backward hook for grad_input is called before "
RuntimeError: Module backward hook for grad_input is called before the grad_output one. This happens because the gradient in your nn.Module flows to the Module's input without passing through the Module's output. Make sure that the output depends on the input and that the loss is computed based on the output.